{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepbanerjee-git/JNCLectures_Intro_to_ML/blob/main/Week12/2025/Lec12_adaboost_m1_cleaned_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f672ad9d",
      "metadata": {
        "id": "f672ad9d"
      },
      "source": [
        "# AdaBoost.M1 from Scratch\n",
        "This notebook demonstrates a simple implementation of the AdaBoost.M1 algorithm using decision stumps as weak learners.\n",
        "\n",
        "We'll use the **Iris dataset** (binary classification: *versicolor* vs *virginica*) and evaluate how boosting improves model performance.\n",
        "\n",
        "### References:\n",
        "- Dataset: [UCI Iris Dataset on Kaggle](https://www.kaggle.com/datasets/uciml/iris)\n",
        "- Algorithm: *The Elements of Statistical Learning* by Hastie, Tibshirani, Friedman\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daee22cd",
      "metadata": {
        "id": "daee22cd"
      },
      "source": [
        "## AdaBoost.M1 Algorithm Steps\n",
        "1. **Initialize** all observation weights equally.\n",
        "2. For each round *m = 1 to M*:\n",
        "   - Sample training data using current weights.\n",
        "   - Train a weak learner (e.g., decision stump).\n",
        "   - Compute the error rate and corresponding alpha value.\n",
        "   - Update observation weights: increase for misclassified samples.\n",
        "   - Normalize weights.\n",
        "3. Final model aggregates the weighted predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "880a6245",
      "metadata": {
        "id": "880a6245"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b592aa4",
      "metadata": {
        "id": "3b592aa4"
      },
      "source": [
        "## Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3063b03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3063b03",
        "outputId": "9c027423-2d56-46a6-f9b8-b960967624a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris(as_frame=True)\n",
        "df = iris.frame\n",
        "df.rename(columns={'target': 'Label'}, inplace=True)\n",
        "df['Label'] = df['Label'].map(dict(enumerate(iris.target_names)))\n",
        "\n",
        "# Filter only versicolor and virginica\n",
        "df = df[df['Label'].isin(['versicolor', 'virginica'])].copy()\n",
        "\n",
        "# Relabel as +1 and -1\n",
        "df['Label'] = df['Label'].map({'versicolor': 1, 'virginica': -1})\n",
        "\n",
        "X = df.drop('Label', axis=1)\n",
        "y = df['Label']\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8322ee9d",
      "metadata": {
        "id": "8322ee9d"
      },
      "source": [
        "## Train a Weak Learner (Base Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c51dbb54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c51dbb54",
        "outputId": "381963d6-0b6d-4f15-d9d3-e124b1484820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Base): 0.9423\n"
          ]
        }
      ],
      "source": [
        "base_tree = DecisionTreeClassifier(max_depth=1, random_state=0)\n",
        "base_tree.fit(X, y)\n",
        "y_pred = base_tree.predict(X)\n",
        "f1_base = f1_score(y, y_pred)\n",
        "print(f\"F1 Score (Base): {f1_base:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a98881d0",
      "metadata": {
        "id": "a98881d0"
      },
      "source": [
        "## Define Boosting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "933441ad",
      "metadata": {
        "id": "933441ad"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(42)\n",
        "\n",
        "def get_bootstrap_data(X_train, y_train, weights):\n",
        "    indices = X_train.index\n",
        "    sampled_indices = rng.choice(indices, size=len(indices), replace=True, p=weights)\n",
        "    return X_train.loc[sampled_indices], y_train.loc[sampled_indices], sampled_indices\n",
        "\n",
        "def get_updated_weights(y_true, y_pred, weights):\n",
        "    misclassified = (y_true != y_pred)\n",
        "    err = np.dot(weights, misclassified)\n",
        "    alpha = np.log((1 - err) / err)\n",
        "    weights = weights * np.exp(alpha * misclassified)\n",
        "    return weights / weights.sum(), alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6528731e",
      "metadata": {
        "id": "6528731e"
      },
      "source": [
        "## Run AdaBoost.M1 for M Iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "467645ca",
      "metadata": {
        "id": "467645ca"
      },
      "outputs": [],
      "source": [
        "M = 10  # number of boosting rounds\n",
        "n = len(X)\n",
        "weights = np.ones(n) / n\n",
        "alpha_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "for m in range(M):\n",
        "    X_bs, y_bs, _ = get_bootstrap_data(X, y, weights)\n",
        "    clf = DecisionTreeClassifier(max_depth=1, random_state=0)\n",
        "    clf.fit(X_bs, y_bs)\n",
        "    y_pred_m = clf.predict(X)\n",
        "    weights, alpha = get_updated_weights(y, y_pred_m, weights)\n",
        "    alpha_list.append(alpha)\n",
        "    y_pred_list.append(y_pred_m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46321f02",
      "metadata": {
        "id": "46321f02"
      },
      "source": [
        "## Final Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fede3caf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fede3caf",
        "outputId": "7e9514eb-16cf-46e7-d693-94e68d8c7f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score after Boosting: 0.9592\n"
          ]
        }
      ],
      "source": [
        "def get_final_prediction(alpha_list, y_pred_list):\n",
        "    final_score = np.sign(np.sum([a * y for a, y in zip(alpha_list, y_pred_list)], axis=0))\n",
        "    return final_score\n",
        "\n",
        "y_final = get_final_prediction(alpha_list, y_pred_list)\n",
        "f1_boosted = f1_score(y, y_final)\n",
        "print(f\"F1 Score after Boosting: {f1_boosted:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}