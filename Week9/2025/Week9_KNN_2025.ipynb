{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNupl8oV/e2zu5PFRNs+0YN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepbanerjee-git/JNCLectures_Intro_to_ML/blob/main/Week9/2025/Week9_KNN_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbour (k-NN) Classification\n",
        "\n",
        "**Overview:**\n",
        "The k-NN algorithm is a simple, non-parametric, and lazy learning method used for classification. It works by:\n",
        "\n",
        "1. **Storing** all available cases and class labels.\n",
        "2. **Computing distances** between a new data point and the stored cases (commonly using the Euclidean distance).\n",
        "3. **Selecting the 'k' closest points** (neighbors) based on the computed distance.\n",
        "4. **Assigning the class** to the new data point by majority voting among the k nearest neighbors.\n",
        "\n",
        "**How It Works:**\n",
        "- **Training Phase:** The algorithm simply stores the training data.\n",
        "- **Prediction Phase:** For each test instance, compute the distance to every training instance, select the k nearest ones, and use their labels to determine the most common class (majority vote).\n",
        "\n",
        "**Advantages:**\n",
        "- Easy to implement and understand.\n",
        "- Non-parametric: It makes no assumptions about the underlying data distribution.\n",
        "- Flexible: Can work with any number of classes.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Prediction can be computationally expensive as it involves calculating the distance to every training sample.\n",
        "- Sensitive to the choice of k and the distance metric.\n",
        "- Performance may degrade with high-dimensional data (curse of dimensionality).\n"
      ],
      "metadata": {
        "id": "GKjB5iBLsC5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V6Cz34VtOzj",
        "outputId": "b3ff61e4-98b3-49ac-d848-f171ad46e236"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'target',\n",
              " 'frame',\n",
              " 'target_names',\n",
              " 'DESCR',\n",
              " 'feature_names',\n",
              " 'filename',\n",
              " 'data_module']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH0HlCVltU3L",
        "outputId": "1c0191b2-acfb-4460-f271-49c8c90c856b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (50, 4) (100,) (50,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    \"\"\"Compute the Euclidean distance between two vectors.\"\"\"\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k=3):\n",
        "    \"\"\"\n",
        "    Predict class labels for the test data using a functional style k-NN.\n",
        "\n",
        "    Parameters:\n",
        "      X_train : np.array, shape (n_samples, n_features) - training features.\n",
        "      y_train : np.array, shape (n_samples,) - training labels.\n",
        "      X_test  : np.array, shape (n_samples, n_features) - test features.\n",
        "      k       : int - number of nearest neighbors to use.\n",
        "\n",
        "    Returns:\n",
        "      predictions : np.array, shape (n_samples,) - predicted class labels.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for x in X_test:\n",
        "        # Compute distances from x to every training sample\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in X_train]\n",
        "        # Get indices of the k closest training samples\n",
        "        k_indices = np.argsort(distances)[:k]\n",
        "        # Get the corresponding labels for these indices and convert to a NumPy array\n",
        "        k_nearest_labels = np.array([y_train[i] for i in k_indices])\n",
        "        # Use scipy.stats.mode with axis=None to determine the most common label\n",
        "        common_label = np.atleast_1d(st.mode(k_nearest_labels, axis=None).mode)[0]\n",
        "        predictions.append(common_label)\n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "jq26Fgc3rk-h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note:**  \n",
        "In some versions of SciPy, `st.mode` may return a scalar when given a one-dimensional array. To avoid indexing errors, we wrap the result using `np.atleast_1d`, ensuring that the mode is always returned as an array.\n"
      ],
      "metadata": {
        "id": "bJU3nTwUvKab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class labels using functional k-NN\n",
        "predictions = knn_predict(X_train, y_train, X_test, k=3)\n",
        "\n",
        "# Evaluate the classifier's accuracy\n",
        "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
        "print(\"Class-Based k-NN Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F00PWGHtt3sO",
        "outputId": "56daaee6-39e0-424e-80f7-b1404dd3313b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class-Based k-NN Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement using OOPs:"
      ],
      "metadata": {
        "id": "brgDfj1CthL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    \"\"\"Compute the Euclidean distance between two vectors.\"\"\"\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        \"\"\"\n",
        "        Initialize the k-NN classifier.\n",
        "\n",
        "        Parameters:\n",
        "          k : int - number of nearest neighbors to use.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Store the training data.\n",
        "\n",
        "        Parameters:\n",
        "          X : np.array, shape (n_samples, n_features) - training features.\n",
        "          y : np.array, shape (n_samples,) - training labels.\n",
        "        \"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for the test data.\n",
        "\n",
        "        Parameters:\n",
        "          X : np.array, shape (n_samples, n_features) - test features.\n",
        "\n",
        "        Returns:\n",
        "          predictions : np.array, shape (n_samples,) - predicted class labels.\n",
        "        \"\"\"\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Compute distances from x to all training samples\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        # Get indices of the k nearest training samples\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        # Get the labels of these k nearest samples\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Use scipy.stats.mode to determine the most common label\n",
        "        return np.atleast_1d(st.mode(k_nearest_labels, axis=None).mode)[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wgcnTG0Crk7l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the k-NN classifier with k=3\n",
        "knn = KNN(k=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "predictions = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier's accuracy\n",
        "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
        "print(\"Class-Based k-NN Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uf3ri8Prk4o",
        "outputId": "f196ea3d-aa04-44c7-df57-924a81c52537"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class-Based k-NN Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7Gu0US6rk11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0hUdU9hrkzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqAWYWQmrkwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTZwwhPnrkuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fq0Ikygsrkru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTywjF8urkJ8"
      },
      "outputs": [],
      "source": []
    }
  ]
}